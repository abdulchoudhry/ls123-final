{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c62232",
   "metadata": {},
   "source": [
    "# Legal Studies 123 Final Project #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09857564",
   "metadata": {},
   "source": [
    "## Gentrification & Crime in New York City Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45efaa6",
   "metadata": {},
   "source": [
    "### Project Members: Abdul Choudhry, Sammy Chean-Udell, Christian Gutierrez, Louisa Ng ###\n",
    "### Group 10 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c968a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/_xqjb5bs6xs2y8pbvk95_q680000gn/T/ipykernel_2898/3782294420.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import json\n",
    "import random\n",
    "from random import sample\n",
    "from folium.plugins import HeatMap, HeatMapWithTime\n",
    "import geojson\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d89a4b",
   "metadata": {},
   "source": [
    "## Part 1: Exploratory Data Analysis (EDA) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 100,000 rows of the large NYPD arrests dataset for efficiency\n",
    "arrests = pd.read_csv(\"NYPD_arrests_sampled100000.csv\").drop(columns = [\"Unnamed: 0\"])\n",
    "arrests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e03a2",
   "metadata": {},
   "source": [
    "**In order to initially perform our exloratory data analysis, our first step was to take a random sample of 100,000 rows of the larger New York City arrest dataset, which had about one million rows of data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests[\"year\"] = arrests[\"ARREST_DATE\"].str.extract(\"\\d{2}\\/\\d{2}\\/(\\d{4})\")\n",
    "arrests[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23100ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lon_Lat column contains string type objects, but we want a POINT geometry object \n",
    "type(arrests[\"Lon_Lat\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts Lon_lat column from str to POINT objects\n",
    "arrests[\"Point\"] = gpd.GeoSeries.from_wkt(arrests[\"Lon_Lat\"]) \n",
    "arrests = gpd.GeoDataFrame(arrests, geometry=\"Point\", crs={\"init\": \"epsg:4326\"})\n",
    "arrests.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af1818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_columns = [\"PD_CD\", \"PD_DESC\", \"KY_CD\", \"OFNS_DESC\", \"LAW_CODE\", \"LAW_CAT_CD\", \"X_COORD_CD\", \n",
    "               \"Y_COORD_CD\", \"Latitude\", \"Longitude\", \"Lon_Lat\", \"Point\"]\n",
    "arrests = arrests.dropna()\n",
    "\n",
    "# for i in range(len(null_columns)):\n",
    "#     arrests.drop(arrests.index[arrests[null_columns[i]] == 0], inplace = True)\n",
    "\n",
    "# arrests.drop(arrests.index[arrests[\"PD_CD\"] == 0], inplace = True)\n",
    "# arrests.drop(arrests.index[arrests[\"PD_DESC\"] == 0], inplace=True)\n",
    "# arrests.drop(arrests.index[arrests[\"KY_CD\"] == 0], inplace=True)\n",
    "# arrests.drop(arrests.index[arrests[\"OFNS_DESC\"] == 0], inplace=True)\n",
    "# arrests.drop(arrests.index[arrests[\"LAW_CODE\"] == 0], inplace=True)\n",
    "# arrests.drop(arrests.index[arrests[\"LAW_CAT_CD\"] == 0], inplace=True)\n",
    "arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf12fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(arrests[\"Lon_Lat\"].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91996dd1",
   "metadata": {},
   "source": [
    "**Below creates an outline of nyc neighborhoods using different data (this data has neighborhood names within the geojson file, which can be useful for later).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08878028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text file to JSON \n",
    "with open('nyc_neighborhood_outlines2_raw.txt') as json_file:     # name of the text file\n",
    "    data2 = json.load(json_file)\n",
    "\n",
    "# Convert JSON to GeoJson \n",
    "from geojson import dump\n",
    "with open('nyc_neighborhood_outlines2.geojson', mode='w') as f:   # name of the output geojson file\n",
    "   dump(data2, f)\n",
    "\n",
    "nyc_neigh_outlines_json2 = json.load(open(\"nyc_neighborhood_outlines2_raw.txt\"))\n",
    "\n",
    "nyc_coords = [40.6955, -73.9509]\n",
    "m2 = folium.Map(nyc_coords, zoom_start=11)\n",
    "\n",
    "folium.GeoJson(nyc_neigh_outlines_json2, style_function=lambda feature: {\n",
    "    'fillColor': '#ffff00',\n",
    "    'fillOpacity': 0,\n",
    "    'color': 'black',        \n",
    "    'weight': 1\n",
    "    }).add_to(m2)\n",
    "\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef2bcb",
   "metadata": {},
   "source": [
    "**Below uses the same geoJSON data as the plotted outline above and converts that data into a table. We can extract the different names of the neighborhoods from the table below, since we can't do it directly from the map.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adf550",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlines_df = gpd.read_file(\"nyc_neighborhood_outlines2_raw.txt\")\n",
    "outlines_df = outlines_df[[\"neighborhood\", \"geometry\"]]\n",
    "outlines_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fb52c",
   "metadata": {},
   "source": [
    "**Spatial joins table of arrests with the outlines (basically creates a new column in the arrests df that adds the name of the neighborhood of where the arrest occured). We now have an arrests table with both a POINT and the neighborhood that the point falls under.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952dab9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if \"neighborhood\" not in list(arrests.columns):\n",
    "    arrests = arrests.sjoin(outlines_df, how=\"inner\")\n",
    "arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests_columns = list(arrests.columns)\n",
    "arrests_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84360c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.groupby(\"neighborhood\")[\"Point\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee610673",
   "metadata": {},
   "source": [
    "**Loading in the New York City evictions dataset. This dataset serves as our initial gentrification factor that we will analyze in NYC. In other words, for the purpose of our EDA, we aim to analyze the relationship between crime (aka the arrests data that we took a sample of) and evictions in New York City since eviction data tells us which regions in the city are more or less gentrified over the years.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce3fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NY_evictions = pd.read_csv(\"Evictions.csv\")\n",
    "NY_evictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NY_evictions_new = NY_evictions[[\"Executed Date\", \"Residential/Commercial\", \"BOROUGH\", \n",
    "                                \"Eviction Postcode\", \"Eviction/Legal Possession\", \"Latitude\", \n",
    "                                \"Longitude\", \"Council District\", \"BIN\", \"BBL\"]].dropna()\n",
    "NY_evictions_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_g = pd.DataFrame(NY_evictions_new.groupby('BOROUGH').size()).rename(columns = {0: 'Count'}).reset_index()\n",
    "ev_g['PROP'] = [i/sum(ev_g['Count']) for i in ev_g['Count']]\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "sns.barplot(x = 'BOROUGH', y = 'PROP', data = ev_g)\n",
    "plt.ylabel('Proportion')\n",
    "plt.xlabel('Borough')\n",
    "plt.title('Proportion of Evictions Across 5 Boroughs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101a9d8",
   "metadata": {},
   "source": [
    "**Created a new table with three columns: Borough, Count, and Proportion. Count is the number of times each Borough appears in the original table and Proportion is the Proportion of each count to the total number of rows in the original table. The above graph shows viewers the proportion of evictions across the five boroughs of NYC. As we can see, the Bronx borough has the highest proportion of evictions over the years.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "residential_versus_commercial = NY_evictions_new.groupby(\"Residential/Commercial\")[\"Executed Date\"].count()\n",
    "residential_versus_commercial.plot(kind = \"bar\", figsize = (8, 6))\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Evictions\")\n",
    "plt.xticks(rotation = 360)\n",
    "plt.suptitle(\"Number of Residential vs. Commercial Evictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f98cb",
   "metadata": {},
   "source": [
    "**The above plot shows the number of residential versus commercial evictions that occurred throughout the years in NYC. Clearly, the number of residential evictions outweigh the commercial ones and this is important to note because individuals from historically lower socioeconomic backgrounds and marginalized races live in residential areas and in general it is more likely for individuals to be evicted rather than commercial businesses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the number of evictions per borough in 2019\n",
    "\n",
    "NY_evictions_new[\"year\"] = NY_evictions_new[\"Executed Date\"].str.extract(\"\\d{2}\\/\\d{2}\\/(\\d{4})\")\n",
    "evictions_2019 = NY_evictions_new[NY_evictions_new[\"year\"] == \"2019\"]\n",
    "cleaned = pd.DataFrame(evictions_2019.groupby(\"BOROUGH\").size()).rename(columns = {0: \"count\"}).reset_index()\n",
    "\n",
    "arrests_19 = arrests[arrests[\"year\"] == \"2019\"]\n",
    "\n",
    "cleaned_arrests = pd.DataFrame(arrests_19.groupby(\"ARREST_BORO\").size()).rename(columns = {0: \"count\"}).reset_index()\n",
    "\n",
    "ax = plt.subplots(figsize = (15, 10))\n",
    " \n",
    "ax = sns.barplot(x=cleaned[\"BOROUGH\"], y=cleaned[\"count\"], color = 'yellow')\n",
    "ax = sns.barplot(x=cleaned_arrests[\"ARREST_BORO\"], y=cleaned_arrests[\"count\"], color = 'black')\n",
    " \n",
    "ax.set(xlabel=\"Borough\", ylabel=\"Number of Arrests or Evictions\", \n",
    "       title = \"Number of Evictions and Arrests per Borough in 2019\")\n",
    "ax.set_xticklabels([\"Bronx\",'Brooklyn','Manhattan','Queens', \"Staten Island\"])\n",
    "\n",
    "plt.legend(labels = [\"Evictions (Yellow)\", \"Arrests (Black)\"], prop = {\"size\": 15})\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00e9c4",
   "metadata": {},
   "source": [
    "**According to the layered bar plot above, viewers are able to see the relationship between the number of arrests and number of evictions per borough in NYC in the year 2019. We decided to analyze this relationship in the year 2019 because it was the last unbiased year before the pandemic began, so it did not skew or visualization. We can see that there were far more evictions (yellow) than arrests or crimes (black) in 2019 per borough; however, can make the conclusion that both the Brooklyn and Manhattan boroughs have the highest number of evictions and arrests in 2019 compared to the rest of the boroughs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a5c4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrests_copy = arrests\n",
    "arrests_copy[\"ARREST_BORO\"] = arrests_copy[\"ARREST_BORO\"].replace({\"B\": \"BRONX\", \"S\": \"STATEN ISLAND\", \n",
    "                                                                   \"K\": \"BROOKLYN\", \"M\": \"MANHATTAN\", \n",
    "                                                                   \"Q\": \"QUEENS\"}).rename({\"ARREST_BORO\": \"BOROUGH\"})\n",
    "arrests_copy = arrests_copy.rename(columns = {\"ARREST_BORO\": \"BOROUGH\"})\n",
    "arrests_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests2 = arrests\n",
    "arrests2['ARREST_BORO'] = arrests['ARREST_BORO'].replace({'B': 'BRONX', 'S': 'STATEN ISLAND', \n",
    "                                                          'K': 'BROOKLYN' , 'M': 'MANHATTAN', 'Q': 'QUEENS'})#.rename({'ARREST_BORO': 'BOROUGH'})                                                \n",
    "arrests2 = arrests2.rename(columns = {'ARREST_BORO' : 'BOROUGH'})\n",
    "arrests2['ONES'] = np.ones(len(arrests2))\n",
    "\n",
    "\n",
    "grouped = arrests2.groupby(['year', 'BOROUGH']).sum().reset_index()\n",
    "# grouped[\"year\"] = list(arrests2[\"year\"].values)\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "sns.lineplot(x = \"year\", y = 'ONES', data = grouped, hue = 'BOROUGH')\n",
    "plt.ylabel('Number of Crimes Committed')\n",
    "plt.xlabel('Year')\n",
    "plt.title('Number of Crimes Committed Across All 5 Boroughs From 2006-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9937c511",
   "metadata": {},
   "source": [
    "**Plotted above is the number of crimes committed over the last 14 years across all 5 boroughs. The ones column represents a counter for each row so that, when summing in the grouby step, each borough will have an accurate number of crimes per year. Additionally, we can see that all five boroughs share the same downward trend as a function of time. We can also note that Staten Island has a significantly lower number of crimes compared to the rest of the boroughs and we believe this is because of its low population size compared to other boroughs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbcfe8",
   "metadata": {},
   "source": [
    "## Analyzing Arrests and Evictions by Race ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f56786",
   "metadata": {},
   "source": [
    "**In this part, we filtered the arrest data by PERP_RACE and neighborhood and took the count of each (PERP_RACE, neighborhood) pair in the race_count table. The Choropleth map has 6 layers (one for each race) and you can select and delect these layers using LayerControl on the top right of the map.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d963e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests[\"PERP_RACE\"].value_counts().drop(index=[\"UNKNOWN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop \"UNKNOWN\" race because we want to focus on known races\n",
    "races_list = arrests[\"PERP_RACE\"].value_counts().drop(index=[\"UNKNOWN\"]).index.to_list()\n",
    "races_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_race = arrests[\"PERP_RACE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods = arrests[\"neighborhood\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_count = arrests[[\"neighborhood\", \"PERP_RACE\"]].value_counts().reset_index()\n",
    "race_count = race_count[race_count[\"PERP_RACE\"].isin(races_list)].reset_index().rename(columns={\"PERP_RACE\":\"race\", 0:\"count\"})\n",
    "race_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cfe14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_count[race_count[\"neighborhood\"] == \"6\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff873d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neighborhood_names = outlines_df[\"neighborhood\"].value_counts().index.tolist()\n",
    "\n",
    "for i in neighborhood_names:\n",
    "    all_i = race_count[race_count[\"neighborhood\"] == i]\n",
    "    for r in races_list:\n",
    "        row = all_i[all_i[\"race\"] == r]\n",
    "        if (all_i.shape[0] == 0) or (row.shape[0] == 0):\n",
    "            row = pd.DataFrame([[i, r, 0]], columns=[\"neighborhood\", \"race\", \"count\"])\n",
    "            race_count = pd.concat([race_count, row])\n",
    "            \n",
    "race_count.sort_values([\"neighborhood\", \"race\"], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229e8a0",
   "metadata": {},
   "source": [
    "**The above table shows the count of each race per neighborhood that appears in the sampled arrests dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36dc5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NY_evictions_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b900b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eviction_lats = list(NY_evictions_new[\"Latitude\"].values)\n",
    "eviction_lons = list(NY_evictions_new[\"Longitude\"].values)\n",
    "eviction_coordinates = [list(x) for x in zip(eviction_lats, eviction_lons)]\n",
    "eviction_coordinates_sample = random.sample(eviction_coordinates, 500)\n",
    "neighborhood_geojson = json.load(open(\"nyc_neighborhood_outlines2_raw.txt\"))\n",
    "\n",
    "bins = np.arange(0, 3501, 500)\n",
    "num_legends = 0\n",
    "nyc_coords = [40.6955, -73.9509]\n",
    "\n",
    "map_by_race = folium.Map(nyc_coords, zoom_start=11)\n",
    "for r in races_list:\n",
    "    c = folium.Choropleth(\n",
    "        geo_data = neighborhood_geojson,\n",
    "        data = race_count[race_count[\"race\"] == r],\n",
    "        columns = [\"neighborhood\", \"count\"],\n",
    "        key_on = \"feature.properties.neighborhood\",\n",
    "        fill_color = \"Greens\",\n",
    "        fill_opacity = 0.9,\n",
    "        line_opacity = 0.5,\n",
    "        legend_name = \"Arrest Counts\",\n",
    "        name = r\n",
    "    )\n",
    "    \n",
    "    for key in c._children:\n",
    "        if num_legends == 0:\n",
    "            num_legends += 1\n",
    "            break\n",
    "        elif key.startswith(\"color_map\"):\n",
    "            del(c._children[key])\n",
    "            \n",
    "    c.add_to(map_by_race)\n",
    "    \n",
    "for i in range(len(eviction_coordinates_sample)):\n",
    "    ## EVICTION INCIDENTS ARE YELLOW ## \n",
    "               folium.CircleMarker(eviction_coordinates_sample[i], icon = folium.Icon(color = \"black\"),\n",
    "                                                                    radius = 4, \n",
    "                                                                    color = \"black\",\n",
    "                                                                    fill_color = \"yellow\",\n",
    "                                                                    fill = True,\n",
    "                                                                    fill_opacity = 1).add_to(map_by_race)\n",
    "    \n",
    "    \n",
    "folium.LayerControl().add_to(map_by_race)\n",
    "    \n",
    "map_by_race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf473c",
   "metadata": {},
   "source": [
    "**The layered choropleth map above shows viewers the relationship between gentrification (specifically eviction) and crime in New York City. As we can see above, viewers are able to adjust the layered choropleth map by race and analyze which neighborhood in NYC has the most crimes by race. Also, the more green a neighborhood is the more crimes it contains and depending on the various races that a user has selected he or she is able to analyze which regions of NYC have the most evictions by race. Additionally, the yellow dots on the map represent individual eviction incidents and our visualization shows that most evictions occur in the Bronx borough which validates the findings of our layered bar plot in previous cells. Lastly, each yellow dot is taken from a random sample of eviction coordinate pairs in order to prevent the choropleth map from overloading with too many data points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd10e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_obj = folium.Map(location = nyc_coords, zoom_start = 10)\n",
    "heatmap = HeatMap(eviction_coordinates_sample, radius = 17).add_to(heatmap_obj)\n",
    "heatmap_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a53a2",
   "metadata": {},
   "source": [
    "**According to the heatmap above, we are able to confirm our findings from the layered choropleth map above and see that most of the data (which consist of the eviction incidents) are clusted around the Bronx borough, which is illustrated by the \"red\" segment on the top of the heatmap.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCoordinates(df, borough):\n",
    "    \"\"\"\n",
    "    Function that takes in the NYC Eviction or crime dataset and\n",
    "    one of the five boroughs in NYC and returns a sample of\n",
    "    the latitude/longitude pairs for that specific borough.\n",
    "    \"\"\"\n",
    "    eviction_df = df[df[\"BOROUGH\"] == borough]\n",
    "    borough_lat = list(eviction_df[\"Latitude\"].values)\n",
    "    borough_lon = list(eviction_df[\"Longitude\"].values)\n",
    "    borough_lat_lon_pairs = [list(x) for x in zip(borough_lat, borough_lon)]\n",
    "    return random.sample(borough_lat_lon_pairs, 100)\n",
    "\n",
    "brooklyn_coords = getCoordinates(NY_evictions_new, \"BROOKLYN\")\n",
    "bronx_coords = getCoordinates(NY_evictions_new, \"BRONX\")\n",
    "staten_island_coords = getCoordinates(NY_evictions_new, \"STATEN ISLAND\")\n",
    "queens_coords = getCoordinates(NY_evictions_new, \"QUEENS\")\n",
    "manhattan_coords = getCoordinates(NY_evictions_new, \"MANHATTAN\")\n",
    "\n",
    "folium_obj = folium.Map(location = nyc_coords, zoom_start = 11)\n",
    "\n",
    "for i in range(len(brooklyn_coords)):\n",
    "    ## BROOKLYN IS BLACK ## \n",
    "               folium.CircleMarker(brooklyn_coords[i], icon = folium.Icon(color = \"black\"),\n",
    "                                                                    radius = 4, \n",
    "                                                                    color = \"black\",\n",
    "                                                                    fill_color = \"black\",\n",
    "                                                                    fill = True,\n",
    "                                                                    fill_opacity = 1).add_to(folium_obj)\n",
    "        \n",
    "for i in range(len(bronx_coords)):\n",
    "    ## BRONX IS RED ##\n",
    "             folium.CircleMarker(bronx_coords[i], icon = folium.Icon(color = \"black\"),\n",
    "                                                                    radius = 4, \n",
    "                                                                    color = \"black\",\n",
    "                                                                    fill_color = \"red\",\n",
    "                                                                    fill = True,\n",
    "                                                                    fill_opacity = 1).add_to(folium_obj)\n",
    "    \n",
    "for i in range(len(staten_island_coords)):\n",
    "    ## STATEN ISLAND IS BLUE ##\n",
    "               folium.CircleMarker(staten_island_coords[i], icon = folium.Icon(color = \"black\"),\n",
    "                                                                    radius = 4, \n",
    "                                                                    color = \"black\",\n",
    "                                                                    fill_color = \"blue\",\n",
    "                                                                    fill = True,\n",
    "                                                                    fill_opacity = 1).add_to(folium_obj)\n",
    "for i in range(len(queens_coords)):\n",
    "    ## QUEENS IS GREEN ##\n",
    "               folium.CircleMarker(queens_coords[i], icon = folium.Icon(color = \"black\"),\n",
    "                                                                    radius = 4, \n",
    "                                                                    color = \"black\",\n",
    "                                                                    fill_color = \"green\",\n",
    "                                                                    fill = True,\n",
    "                                                                    fill_opacity = 1).add_to(folium_obj)\n",
    "        \n",
    "for i in range(len(manhattan_coords)):\n",
    "    ## MANHATTAN IS YELLOW ##\n",
    "               folium.CircleMarker(manhattan_coords[i], icon = folium.Icon(color = \"black\"),\n",
    "                                                                    radius = 4, \n",
    "                                                                    color = \"black\",\n",
    "                                                                    fill_color = \"yellow\",\n",
    "                                                                    fill = True,\n",
    "                                                                    fill_opacity = 1).add_to(folium_obj)\n",
    "folium_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ea626",
   "metadata": {},
   "source": [
    "**The above plot shows viewers the number of evictions per borough in New York City while each borough is color-coded a specific color. Brooklyn is Black, Bronx is Red, Staten Island is Blue, Queens is Green, and Manhattan is Yellow. Additionally, each point on the folium map is taken from a random sample of coordinate pairs in order to prevent the map from overloading with too many data points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_lats = list(arrests[\"Latitude\"].values)\n",
    "arrest_lons = list(arrests[\"Longitude\"].values)\n",
    "arrest_coordinates = [list(x) for x in zip(arrest_lats, arrest_lons)]\n",
    "arrest_coordinates_sample = random.sample(arrest_coordinates, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd95f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folium_obj = folium.Map(location = nyc_coords, zoom_start = 11)\n",
    "\n",
    "for i in range(len(arrest_coordinates_sample)):\n",
    "    ## ARRESTS ARE YELLOW ## \n",
    "               folium.CircleMarker(arrest_coordinates_sample[i], icon = folium.Icon(color = \"black\"),\n",
    "                                                                    radius = 4, \n",
    "                                                                    color = \"black\",\n",
    "                                                                    fill_color = \"yellow\",\n",
    "                                                                    fill = True,\n",
    "                                                                    fill_opacity = 1).add_to(folium_obj)\n",
    "for i in range(len(eviction_coordinates_sample)):\n",
    "    ## EVICTIONS ARE RED ## \n",
    "               folium.CircleMarker(eviction_coordinates_sample[i], icon = folium.Icon(color = \"black\"),\n",
    "                                                                    radius = 4, \n",
    "                                                                    color = \"black\",\n",
    "                                                                    fill_color = \"red\",\n",
    "                                                                    fill = True,\n",
    "                                                                    fill_opacity = 1).add_to(folium_obj)\n",
    "folium_obj   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd646839",
   "metadata": {},
   "source": [
    "**This final map above shows viewers our underlying hypothesis for our project. Essentially, this map shows the relationship between crime and gentrification (specifically eviction) in New York City. In our proposal we hypothesized how eviction is one of the most prevalent factors of gentrification and can see that both eviction and arrests are clustered together in specific regions of New York City such as the Bronx. Later on in our analysis, we hope to analyze gentrification and eviction displacements as a function of time and predict gentrification by crime. We may also explore the \"racial tipping point\" where an area's non-white population reaches a certain threshold such that white flight exists, which is a movement of white residents to the outskirts of urban areas or regions where the \"racial tipping point\" occurrs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257b8e1",
   "metadata": {},
   "source": [
    "## Part 2: Modeling & Prediction ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f85365",
   "metadata": {},
   "outputs": [],
   "source": [
    "NY_evictions_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88011c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# add month column to arrest table \n",
    "arrests[\"month\"] = arrests[\"ARREST_DATE\"].str.extract(r\"(\\d{2})\\/\\d{2}\\/\\d{4}\").astype(int)\n",
    "\n",
    "# change data type of \"year\" column from string to int\n",
    "arrests[\"year\"] = arrests[\"year\"].astype(int)\n",
    "arrests.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fdaf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds \"crime_count\" column to arrests table, where crime count represents\n",
    "# the total number of crimes per month per year\n",
    "crime_count = arrests[[\"year\", \"month\", \"neighborhood\"]].value_counts().reset_index().rename(columns={0:\"crime_count\"})\n",
    "arrests = arrests.merge(crime_count, on=[\"year\", \"month\", \"neighborhood\"])\n",
    "arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7872eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NY_evictions_new.rename(columns = {\"BOROUGH\": \"ARREST_BORO\"}, inplace = True) # unnecessary to rename this column\n",
    "NY_evictions_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc853bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our ultimate goal is to join eviction data with ny arrest data on neighborhoods, so we will \n",
    "# place each eviction in a neighborhood based on latitude and longitude columns\n",
    "\n",
    "NY_evictions_new = gpd.GeoDataFrame(NY_evictions_new, geometry=gpd.points_from_xy(\n",
    "    NY_evictions_new.Longitude, NY_evictions_new.Latitude))\n",
    "nyc_outlines_table = gpd.read_file(\"nyc_neighborhood_outlines2.geojson\")\n",
    "\n",
    "if \"neighborhood\" not in list(NY_evictions_new.columns):\n",
    "    NY_evictions_new = NY_evictions_new.sjoin(nyc_outlines_table)\n",
    "NY_evictions_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"month\" column to NY_evictions new \n",
    "NY_evictions_new[\"month\"] = NY_evictions_new[\"Executed Date\"].str.extract(r\"(\\d{2})\\/\\d{2}\\/\\d{4}\").astype(int)\n",
    "\n",
    "# convert year column to int values \n",
    "NY_evictions_new[\"year\"] = NY_evictions_new[\"year\"].astype(int)\n",
    "\n",
    "# create counts table of number of evictions based on month, year, borough, and residential/commericial \n",
    "evictions_count = NY_evictions_new[[\"month\", \"year\", \"neighborhood\", \"Residential/Commercial\"]].value_counts().reset_index().rename(columns={0:\"eviction_count\"})\n",
    "\n",
    "evictions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge eviction counts to arrests table on the month, year, and ny neighborhood\n",
    "arrests = arrests.merge(evictions_count, on=[\"month\", \"year\", \"neighborhood\"])\n",
    "print(arrests.columns)\n",
    "arrests.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NYC housing data, clean and merge with arrests table based on date, month, and neighborhood \n",
    "\n",
    "ny_housing = pd.read_csv(\"Housing_New_York_Units_by_Building.csv\")\n",
    "ny_housing.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new month and year columns\n",
    "\n",
    "ny_housing[\"month\"] = ny_housing[\"Project Start Date\"].str.extract(r\"(\\d{2})\\/\\d{2}\\/\\d{4}\").astype(int)\n",
    "ny_housing[\"year\"] = ny_housing[\"Project Start Date\"].str.extract(r\"\\d{2}\\/\\d{2}\\/(\\d{4})\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs following code once\n",
    "# code does a spatial join of ny_housing data with corresponding neighborhoods\n",
    "\n",
    "if \"neighborhood\" not in list(ny_housing.columns):\n",
    "    ny_housing = gpd.GeoDataFrame(ny_housing, geometry=gpd.points_from_xy(\n",
    "    ny_housing.Longitude, ny_housing.Latitude))\n",
    "    ny_housing = ny_housing.sjoin(nyc_outlines_table)\n",
    "\n",
    "ny_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21355327",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccb088",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_income_housing_types = ['Extremely Low Income Units', 'Very Low Income Units', 'Low Income Units', \n",
    "                    'Moderate Income Units', 'Middle Income Units', \n",
    "                    \"Other Income Units\", \"Studio Units\", \"1-BR Units\", \"2-BR Units\", \n",
    "                    \"3-BR Units\", \"4-BR Units\", \"5-BR Units\", \"6-BR+ Units\", \n",
    "                    \"Unknown-BR Units\", \"Counted Rental Units\", \"Counted Homeownership Units\", \n",
    "                    \"All Counted Units\", \"Total Units\"]\n",
    "\n",
    "# ny_housing_counts = ny_housing[[\"month\", \"year\", \"neighborhood\"] + ny_income_housing_types].value_counts().reset_index(\n",
    "# ).rename(columns={0:\"count\"})\n",
    "# ny_housing_counts\n",
    "\n",
    "ny_housing_counts = ny_housing.groupby([\"month\", \"year\", \"neighborhood\"]).sum().reset_index()\n",
    "ny_housing_counts = ny_housing_counts[[\"month\", \"year\", \"neighborhood\"] + ny_income_housing_types]\n",
    "ny_housing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN MERGE ONCE\n",
    "\n",
    "arrests = arrests.merge(ny_housing_counts, on=[\"month\", \"year\", \"neighborhood\"])\n",
    "arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da432c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests[[\"X_COORD_CD\", \"Y_COORD_CD\", \"AGE_GROUP\", \"PERP_SEX\", \"PERP_RACE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58510cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests_quantitative = arrests[[\"PD_CD\", \"KY_CD\", \"ARREST_PRECINCT\", \"month\", \"year\",\n",
    "                               \"eviction_count\", \"X_COORD_CD\", \"Y_COORD_CD\", \n",
    "                               \"Latitude\", \"Longitude\",\"Extremely Low Income Units\", \n",
    "                               \"Very Low Income Units\", \"Low Income Units\", \"Moderate Income Units\", \n",
    "                               \"Middle Income Units\", \"Other Income Units\", \n",
    "                                \"Studio Units\", \"1-BR Units\", \"2-BR Units\", \n",
    "                                \"3-BR Units\", \"4-BR Units\", \"5-BR Units\", \"6-BR+ Units\", \n",
    "                                \"Unknown-BR Units\", \"Counted Rental Units\", \"Counted Homeownership Units\", \n",
    "                                \"All Counted Units\", \"Total Units\"]]\n",
    "arrests_quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are trying to predict the number of crimes per month per year in NYC so predicted y variable is assigned\n",
    "# to this quantity. \n",
    "\n",
    "# NOTE: the \"crime_count\" column is NOT in the \"arrests_quantitative\" dataframe because this will serve as \n",
    "# our training dataset, and we do not want to include the predicted column (aka \"crime_count\") in the training data. \n",
    "\n",
    "y = list(arrests[\"crime_count\"].values)\n",
    "\n",
    "# standardizing the features in the crimes_df dataset so that they have zero mean and unit variance \n",
    "scaler = StandardScaler()\n",
    "arrests_df_standardized = pd.DataFrame(scaler.fit_transform(arrests_quantitative.values), \n",
    "                                      columns=arrests_quantitative.columns, index=arrests_quantitative.index)\n",
    "arrests_df_standardized.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b760f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(arrests_df_standardized, y, train_size=0.80, test_size=0.20)\n",
    "\n",
    "# further partitioning the data into a training, test, and validation set.\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, train_size = 0.70, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0aa828",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53676e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests_quant_with_crime = arrests[[\"PD_CD\", \"KY_CD\", \"ARREST_PRECINCT\", \"month\", \"year\",\n",
    "                               \"eviction_count\", \"crime_count\", \"X_COORD_CD\", \"Y_COORD_CD\", \n",
    "                               \"Latitude\", \"Longitude\",\"Extremely Low Income Units\", \n",
    "                               \"Very Low Income Units\", \"Low Income Units\", \"Moderate Income Units\", \n",
    "                               \"Middle Income Units\", \"Other Income Units\", \n",
    "                                \"Studio Units\", \"1-BR Units\", \"2-BR Units\", \n",
    "                                \"3-BR Units\", \"4-BR Units\", \"5-BR Units\", \"6-BR+ Units\", \n",
    "                                \"Unknown-BR Units\", \"Counted Rental Units\", \"Counted Homeownership Units\", \n",
    "                                \"All Counted Units\", \"Total Units\"]]\n",
    "arrests_quant_with_crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4afdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests_quant_with_crime_standardized = pd.DataFrame(scaler.fit_transform(arrests_quant_with_crime.values), \n",
    "                                      columns=arrests_quant_with_crime.columns, index=arrests_quant_with_crime.index)\n",
    "\n",
    "arrests_quant_with_crime_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82adf8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = arrests_quant_with_crime_standardized.corr()\n",
    "\n",
    "# NOTE: all values in the df below are between -1 and 1.\n",
    "# In other words, -1 < r < 1, where r is the correlation coefficient of the features we plan to use \n",
    "# prior to any feature selection. \n",
    "\n",
    "correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 18))\n",
    "sns.heatmap(correlation_df, annot = True)\n",
    "plt.title(\"Correlation Matrix for Gentrification Features Prior to Feature Selection\", fontdict = {'fontsize': 27})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494833d4",
   "metadata": {},
   "source": [
    "In the above heatmap shows the correlation between the features we plan to feed into our model BEFORE we do any feature selection techniques and choose our preconceived \"best\" gentrification features to predict number of crimes per month per year in NYC. We can already see above that the \"crime_count\" feature has a correlation coefficient of 0.23 with respect to the \"eviction_count\" feature, which is just one feature of gentrification we are analyzing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92becbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corr = abs(correlation_df[\"crime_count\"])\n",
    "target_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decent_features = list(target_corr[target_corr > 0.05].index)\n",
    "decent_features.remove(\"crime_count\")\n",
    "decent_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b5896",
   "metadata": {},
   "source": [
    "In order to build an accurate model, we needed to clean our data. Initially, we dropped all null values from our tables. Although dropping null values runs the risk of losing patterns hidden in the data, we felt confident that no meaningful pattern existed within the null values for the sake of our goal. Our resulting main dataframe of New York arrest reports was stored in a variable called ‘arrests’. We merge the arrests table with two other imported datasets in order to provide more potential features to choose from during our feature selection process. The first dataset contained data on residential and commercial evictions in New York starting from 2017. We converted the Latitude and Longitude columns into point geometry objects in order to do a spatial join with geojson data that places each eviction property in its corresponding New York neighborhood. After, we extracted the month and year of each eviction from the date column and converted to values to integers. With the resulting New York evictions table, we grouped the table by year, month, and neighborhood using the value_counts method to get the total number of evictions per (month, year, neighborhood) combination and merged the resulting dataframe to the arrests table on the same columns. The second dataset contained data on construction of housing projects in New York from 2014 to 2021. Similarly, we placed each property in its corresponding neighborhood using a spatial join, extracted the month and year values from the date column, and created a counts table based on the different (month, year, neighborhood) combinations. We merged this with the arrests table. Furthermore, in order to ensure that all features would be treated equally by our feature selection methods, we had to standardize them. Standardizing our variables made it possible for our model to consider all attributes on a common scale, allowing it to differentiate between features based solely on their influence on our predicted variable, arrest counts. This allowed for our model to accurately weight each feature against the influence it holds over the feature we are predicting. Lastly, before training our data on different potential models, it was important to discern the most influential features. To do so, we used a correlation matrix and used a cutoff of 0.05 to weed out the least important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred, actual):\n",
    "    return np.sqrt(np.mean((pred - actual) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479bc78",
   "metadata": {},
   "source": [
    "**Ordinary Least Squares regression estimates relationships between independent variables and dependent variables. In order to do this, the model reduces the sums of squares in the difference between observed and predicted values of the dependent variables, generating singular one dimensional lines of best fit. Linear regression is an incredibly efficient regression method, as it is easy to train and easy to adjust when encountering issues due to overfitting. However, Linear Regression is limited by myriad obstacles, inluding its assumption of a linear relationship between independent and dependent variable. This is problematic as data based in real life, such as our gentrification data, is much more complex. Furthermore, if the data has a larger number of parameters than observations, OLS is very likely to overfit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "lin_model_fit = lin_model.fit(X_train, y_train)\n",
    "lin_model_pred = lin_model_fit.predict(X_train)\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(y_train, lin_model_pred)\n",
    "plt.xlabel(\"Actual Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.suptitle(\"Linear Model (OLS)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Linear Regression Model RMSE:\", rmse(lin_model_pred, y_train))\n",
    "print(\"Linear Regression Model Training Accuracy:\", lin_model_fit.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_cross_val_pred = cross_val_predict(lin_model, arrests_df_standardized, y, cv = 4)\n",
    "r2_score(y, linear_cross_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d348fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lin_model = LinearRegression()\n",
    "new_lin_model_fit = new_lin_model.fit(X_train[decent_features], y_train)\n",
    "new_lin_model_pred = new_lin_model_fit.predict(X_train[decent_features])\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.scatter(y_train, new_lin_model_pred)\n",
    "plt.xlabel(\"Actual Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.suptitle(\"New Linear Model (OLS)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"New Linear Regression Model RMSE:\", rmse(new_lin_model_pred, y_train))\n",
    "print(\"New Linear Regression Model Training Accuracy:\", new_lin_model_fit.score(X_train[decent_features], y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using one of the feature selection methods from Lab 14\n",
    "recursive_feature_elimination = RFE(lin_model, n_features_to_select = 7)\n",
    "recursive_feature_elimination.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recursive_feature_elimination.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68015fcc",
   "metadata": {},
   "source": [
    "\"The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1\" (Marshall, Lab 14). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadd05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recursive_feature_elimination.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09529732",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_pred = recursive_feature_elimination.predict(X_train)\n",
    "print(\"RFE Model RMSE:\", rmse(rfe_pred, y_train))\n",
    "print(\"RFE Model Training Accuracy:\", recursive_feature_elimination.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ca91e",
   "metadata": {},
   "source": [
    "**Ridge Regression produces a predictive model by creating a colinear equation weighting features by the influence it holds over our predicted variable. A feature with a heavier weight is seemingly much more influential, while a feature with a lighter weight impacts the predicted variable less. Ridge Regression will help us create an accurate predictive model because it works very well when fed with multiple features. Even though Ridge Regression is a good choice of model, it is important to acknowledge its disadvantages. For instance, a feature may, in code, seem to have no bearing over our predicted variable, yet it could have a larger societal meaning that is not be encoded in the data. This would mean that Ridge Regression assigns a lighter weight to the feature, even though it holds great influence over the predicted variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "ridge_model = ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge_model.predict(X_train)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.scatter(y_train, ridge_pred)\n",
    "plt.xlabel(\"Actual Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.suptitle(\"Ridge Model\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Ridge RMSE:\", rmse(ridge_pred, y_train))\n",
    "print(\"Ridge Training Accuracy:\", ridge_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb475035",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ridge = Ridge()\n",
    "new_ridge_model = new_ridge.fit(X_train[decent_features], y_train)\n",
    "new_ridge_pred = new_ridge_model.predict(X_train[decent_features])\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.scatter(y_train, new_ridge_pred)\n",
    "plt.xlabel(\"Actual Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.suptitle(\"Ridge Model\")\n",
    "plt.show()\n",
    "\n",
    "print(\"New Ridge RMSE:\", rmse(new_ridge_pred, y_train))\n",
    "print(\"New Ridge Training Accuracy:\", new_ridge_model.score(X_train[decent_features], y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeba06b",
   "metadata": {},
   "source": [
    "**Lasso Regression uses a concept called shrinkage to essentially perform feature selection through variable elimination. Shrinkage is the model’s ability to “shrink” all data points based on how similar they are to a central value. Each data point is weighed against the central value, shrinking towards 0 if the value of the data point is distant from the central value. As data points are “shrunk”, features are passively eliminated through their eventual disappearance of associated data. Lasso Regression is useful when the data exhibits high dimensioniality and high correlation. However, in circumstances where neither of those hold, Lasso is not very useful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cross_val_model = LassoCV(cv = 5, max_iter = 10000)\n",
    "lasso_cross_val_fitted = lasso_cross_val_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"LassoCV RMSE:\", rmse(lasso_cross_val_model.predict(X_train), y_train))\n",
    "print(\"LassoCV Training Accuracy: %f\" %lasso_cross_val_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be99b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = tree.DecisionTreeClassifier(random_state = 10)\n",
    "decision_tree_model = decision_tree.fit(X_train[decent_features], y_train)\n",
    "decision_tree_pred = decision_tree_model.predict(X_train[decent_features])\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.scatter(y_train, decision_tree_pred)\n",
    "plt.xlabel(\"Actual Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.suptitle(\"Decision Tree Model\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Decision Tree RMSE:\", rmse(decision_tree_pred, y_train))\n",
    "print(\"Decision Tree Training Accuracy:\", decision_tree_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_decision_tree = tree.DecisionTreeClassifier(random_state = 10)\n",
    "new_decision_tree_model = new_decision_tree.fit(X_train, y_train)\n",
    "new_decision_tree_pred = new_decision_tree_model.predict(X_train)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.scatter(y_train, new_decision_tree_pred)\n",
    "plt.xlabel(\"Actual Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.suptitle(\"New Decision Tree Model\")\n",
    "plt.show()\n",
    "\n",
    "print(\"New Decision Tree RMSE:\", rmse(new_decision_tree_pred, y_train))\n",
    "print(\"New Decision Tree Training Accuracy:\", new_decision_tree_model.score(X_train[decent_features], y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9d209",
   "metadata": {},
   "source": [
    "**In Support Vector Machine Regression, the goal is to find a hyperplane which accurately separates a majority of the data points, creating “classes” based on which side of the hyperplane a data point falls. A hyperplane is a plane which takes on one less dimension than the field it exists in. In order to better understand SVM, consider a line of best fit. In a 2D field, a line of best fit averages the y values to best find a line which either intersects or separates all or most of the data points. If the data is so separated that no line can fit through all points, then the line of best fit becomes a dividing line between data points, categorizing data points based on existing similarities. Now consider the 3D field: a hyperplane is a line of best fit, but in two dimensions, rather than one. SVM is incredibly useful for generalization and is robust to outliers. However, it is very sensitive to noise, as it is difficult to separate the data into classes, making it unideal for certain datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6edaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try a support vector machine (SVM) regression model\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html \n",
    "\n",
    "# https://scikit-learn.org/stable/modules/svm.html \n",
    "\n",
    "SVM_model = make_pipeline(StandardScaler(), SVR(C = 38.42, epsilon = 0.45))\n",
    "\n",
    "# the argument \"SVR\" implies that the SVM model is in fact a regression model\n",
    "# SVR stands for \"Support Vector Regression\"\n",
    "# the \"C\" parameter is the squared L2 regularization penalty paramater in the SVR's underlying implementation.\n",
    "# As the \"C\" parameter increases, it takes more time to train the data. Decreasing it leads to more regularization. \n",
    "# the \"epsilon\" parameter represents the distance away from the training loss function, based on the value of C.\n",
    "\n",
    "SVM_model_fitted = SVM_model.fit(X_train, y_train)\n",
    "SVM_model_pred = SVM_model_fitted.predict(X_train)\n",
    "\n",
    "plt.figure(figsize = (14, 10))\n",
    "plt.scatter(y_train, SVM_model_pred)\n",
    "plt.xlabel(\"Actual Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.title(\"Support Vector Machine Regression (SVR) Model\",  fontdict = {'fontsize': 20})\n",
    "plt.show()\n",
    "\n",
    "print(\"SVM Model RMSE:\", rmse(SVM_model_pred, y_train))\n",
    "print(\"SVM Model Training Accuracy:\", SVM_model.score(X_train, y_train))\n",
    "\n",
    "SVM_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_new_model = make_pipeline(StandardScaler(), SVR(C = 38.42, epsilon = 0.45))\n",
    "\n",
    "SVM_new_model_fitted = SVM_model.fit(X_train[decent_features], y_train)\n",
    "SVM_new_model_pred = SVM_model_fitted.predict(X_train[decent_features])\n",
    "\n",
    "plt.figure(figsize = (14, 10))\n",
    "plt.scatter(y_train, SVM_new_model_pred)\n",
    "plt.xlabel(\"Actual Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.title(\"Support Vector Machine Regression (SVR) Model\",  fontdict = {'fontsize': 20})\n",
    "plt.show()\n",
    "\n",
    "print(\"SVM Model RMSE:\", rmse(SVM_model_pred, y_train))\n",
    "print(\"SVM Model Training Accuracy:\", SVM_model.score(X_train, y_train))\n",
    "\n",
    "SVM_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdc003",
   "metadata": {},
   "source": [
    "**We chose SVM as our final model because it gave the lowest rmse, at 2.635, of all our tested models. Root Mean Square Error essentially calculates how far each data point is from the line of best fit. Therefore, a lower rmse shows smaller residuals which means a more accurate model. Furthermore, SVM’s model score showcases the model’s higher training accuracy when compared to that of the other models. SVM’s score was approximately 76%, while other model scores were as low as 20%. Finally, SVM is able to better control for overfitting by utilizing more training data efficiently.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85e1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996f678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
